<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Research Work</TITLE>
<META name="generator" content="BCL easyConverter SDK 5.0.241">
<META name="author" content="Kumar Ankit">
<STYLE type="text/css">

body {margin-top: 0px;margin-left: 0px;}

#page_1 {position:relative; overflow: hidden;margin: 101px 0px 12px 72px;padding: 0px;border: none;width: 744px;}
#page_1 #id1_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 744px;overflow: hidden;}
#page_1 #id1_2 {border:none;margin: 65px 0px 0px 200px;padding: 0px;border:none;width: 544px;overflow: hidden;}





#page_2 {position:relative; overflow: hidden;margin: 101px 0px 12px 72px;padding: 0px;border: none;width: 744px;}
#page_2 #id2_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 744px;overflow: hidden;}
#page_2 #id2_2 {border:none;margin: 52px 0px 0px 200px;padding: 0px;border:none;width: 544px;overflow: hidden;}





#page_3 {position:relative; overflow: hidden;margin: 101px 0px 12px 72px;padding: 0px;border: none;width: 744px;}
#page_3 #id3_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 744px;overflow: hidden;}
#page_3 #id3_2 {border:none;margin: 52px 0px 0px 200px;padding: 0px;border:none;width: 544px;overflow: hidden;}





#page_4 {position:relative; overflow: hidden;margin: 119px 0px 12px 72px;padding: 0px;border: none;width: 744px;}
#page_4 #id4_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 744px;overflow: hidden;}
#page_4 #id4_2 {border:none;margin: 706px 0px 0px 200px;padding: 0px;border:none;width: 544px;overflow: hidden;}

#page_4 #p4dimg1 {position:absolute;top:219px;left:1px;z-index:-1;width:672px;height:577px;}
#page_4 #p4dimg1 #p4img1 {width:672px;height:577px;}




.ft0{font: 21px 'Arial';line-height: 24px;}
.ft1{font: 16px 'Arial';line-height: 18px;}
.ft2{font: 15px 'Arial';line-height: 17px;}
.ft3{font: 19px 'Arial';line-height: 22px;}
.ft4{font: 13px 'Arial';line-height: 20px;}
.ft5{font: 13px 'Arial';line-height: 16px;}
.ft6{font: italic 15px 'Arial';margin-left: 11px;line-height: 18px;}
.ft7{font: 15px 'Arial';line-height: 18px;}
.ft8{font: italic 15px 'Arial';line-height: 18px;}
.ft9{font: 15px 'Arial';margin-left: 13px;line-height: 18px;}
.ft10{font: 12px 'Arial';line-height: 15px;}
.ft11{font: 15px 'Arial';margin-left: 14px;line-height: 17px;}
.ft12{font: 15px 'Arial';margin-left: 16px;line-height: 18px;}
.ft13{font: 15px 'Arial';margin-left: 13px;line-height: 19px;}
.ft14{font: 15px 'Arial';line-height: 19px;}
.ft15{font: 8px 'Calibri';line-height: 10px;}
.ft16{font: 14px 'Arial';margin-left: 14px;line-height: 18px;}
.ft17{font: 14px 'Arial';line-height: 18px;}
.ft18{font: 13px 'Arial';margin-left: 13px;line-height: 18px;}
.ft19{font: 13px 'Arial';line-height: 18px;}
.ft20{font: 11px 'Arial';line-height: 14px;}
.ft21{font: italic 15px 'Arial';margin-left: 12px;line-height: 18px;}
.ft22{font: 15px 'Arial';margin-left: 13px;line-height: 17px;}
.ft23{font: 15px 'Arial';margin-left: 16px;line-height: 17px;}
.ft24{font: 14px 'Arial';line-height: 19px;}
.ft25{font: 15px 'Arial';margin-left: 12px;line-height: 18px;}
.ft26{font: 14px 'Arial';margin-left: 12px;line-height: 18px;}
.ft27{font: 13px 'Arial';margin-left: 11px;line-height: 18px;}
.ft28{font: 15px 'Arial';margin-left: 14px;line-height: 18px;}
.ft29{font: 15px 'Arial';text-decoration: underline;color: #0563c1;line-height: 18px;}
.ft30{font: 15px 'Arial';margin-left: 11px;line-height: 18px;}
.ft31{font: 14px 'Arial';line-height: 16px;}

.p0{text-align: left;padding-left: 51px;margin-top: 0px;margin-bottom: 0px;}
.p1{text-align: left;padding-left: 296px;margin-top: 13px;margin-bottom: 0px;}
.p2{text-align: left;padding-left: 196px;margin-top: 2px;margin-bottom: 0px;}
.p3{text-align: left;margin-top: 36px;margin-bottom: 0px;}
.p4{text-align: justify;padding-right: 68px;margin-top: 13px;margin-bottom: 0px;}
.p5{text-align: justify;padding-left: 24px;padding-right: 73px;margin-top: 5px;margin-bottom: 0px;text-indent: -24px;}
.p6{text-align: justify;padding-left: 48px;padding-right: 73px;margin-top: 1px;margin-bottom: 0px;text-indent: -24px;}
.p7{text-align: left;padding-left: 48px;padding-right: 73px;margin-top: 0px;margin-bottom: 0px;text-indent: -24px;}
.p8{text-align: justify;padding-left: 48px;padding-right: 73px;margin-top: 0px;margin-bottom: 0px;text-indent: -24px;}
.p9{text-align: justify;padding-left: 24px;padding-right: 73px;margin-top: 17px;margin-bottom: 0px;text-indent: -24px;}
.p10{text-align: justify;padding-left: 48px;padding-right: 74px;margin-top: 0px;margin-bottom: 0px;text-indent: -24px;}
.p11{text-align: left;margin-top: 0px;margin-bottom: 0px;}
.p12{text-align: justify;padding-left: 48px;padding-right: 68px;margin-top: 0px;margin-bottom: 0px;text-indent: -24px;}
.p13{text-align: justify;padding-left: 24px;padding-right: 73px;margin-top: 16px;margin-bottom: 0px;text-indent: -24px;}
.p14{text-align: left;padding-left: 24px;margin-top: 0px;margin-bottom: 0px;}
.p15{text-align: left;padding-left: 24px;margin-top: 2px;margin-bottom: 0px;}
.p16{text-align: left;margin-top: 12px;margin-bottom: 0px;}
.p17{text-align: justify;padding-right: 68px;margin-top: 11px;margin-bottom: 0px;}
.p18{text-align: left;padding-left: 24px;padding-right: 73px;margin-top: 8px;margin-bottom: 0px;text-indent: -24px;}
.p19{text-align: left;padding-left: 24px;padding-right: 74px;margin-top: 0px;margin-bottom: 0px;text-indent: -24px;}
.p20{text-align: left;padding-left: 24px;padding-right: 69px;margin-top: 14px;margin-bottom: 0px;text-indent: -24px;}
.p21{text-align: justify;padding-left: 48px;padding-right: 73px;margin-top: 2px;margin-bottom: 0px;text-indent: -24px;}
.p22{text-align: left;padding-left: 24px;padding-right: 68px;margin-top: 17px;margin-bottom: 0px;text-indent: -24px;}
.p23{text-align: justify;padding-left: 24px;padding-right: 73px;margin-top: 14px;margin-bottom: 0px;text-indent: -24px;}
.p24{text-align: left;padding-left: 24px;padding-right: 73px;margin-top: 16px;margin-bottom: 0px;text-indent: -24px;}
.p25{text-align: left;padding-left: 24px;padding-right: 73px;margin-top: 18px;margin-bottom: 0px;text-indent: -24px;}
.p26{text-align: left;padding-left: 48px;padding-right: 73px;margin-top: 1px;margin-bottom: 0px;text-indent: -24px;}
.p27{text-align: justify;padding-left: 24px;padding-right: 73px;margin-top: 0px;margin-bottom: 0px;text-indent: -24px;}
.p28{text-align: justify;padding-right: 74px;margin-top: 8px;margin-bottom: 0px;}




</STYLE>
</HEAD>

<BODY>
<DIV id="page_1">


<DIV id="id1_1">
<P class="p0 ft0">A <NOBR>Multi-Agent</NOBR> Framework for Automated Agriculture Applications</P>
<P class="p1 ft1">Kumar Ankit</P>
<P class="p2 ft2">Robert Bosch Center for Cyber Physical Systems</P>
<P class="p3 ft3">Basic Research Proposal</P>
<P class="p4 ft4">The popularity of UAVs in scientific data gathering and applications, especially the use of small <NOBR>multi-rotor</NOBR> UAVs is quite widespread. There has also been a sudden spurt of UAV use in niche domains such as agriculture. Agriculturalists are choosing <NOBR>UAV-based</NOBR> field operations and remote sensing over the <NOBR>satellite-based</NOBR> ones, especially for <NOBR>local-scale</NOBR> and high spatiotemporal resolution imagery. However, the combination of aerial survey capabilities of Unmanned Aerial Vehicles with targeted intervention abilities of agricultural Unmanned Ground Vehicles can significantly improve the effectiveness of robotic systems applied to automated agriculture. In this research report a framework for such collaboration has been proposed.</P>
<P class="p5 ft8"><SPAN class="ft5">A)</SPAN><NOBR><SPAN class="ft6">Multi-Agent</SPAN></NOBR> Collaborative Framework for Agriculture: <SPAN class="ft7">IoT networks for farms end up in messy arrangements that need to be put into a systematic framework. At the other end of the spectrum, usage of heavy UGVs compacts the topsoil and hampers its productivity. A hybrid approach combining UAVs and </SPAN><NOBR><SPAN class="ft7">light-weight</SPAN></NOBR><SPAN class="ft7"> UGVs could be employed where UAVs will be the master of the operations (along with standard operations like surveillance, spraying, etc.), with heavier work like tilling and sowing done by the UGVs. Submodules for the framework are listed below</SPAN></P>
<P class="p6 ft7"><SPAN class="ft5">a.</SPAN><NOBR><SPAN class="ft9">Multi-vehicle</SPAN></NOBR> <NOBR>co-ordination:</NOBR> Cooperation between aerial and ground robots undoubtedly offers benefits to many applications, thanks to the complementarity of the characteristics of these robots. UAVs can hover and provide the visionary data to detect and locate the events whereas UGVs can be employed to serve the task with suitable equipment.</P>
<P class="p7 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Mission planner: For a centralized </SPAN><NOBR>multi-agent</NOBR> setup, it is necessary to have a mission planner to demarcate and allocate the tasks in an optimized fashion.</P>
<P class="p8 ft2"><SPAN class="ft10">c.</SPAN><SPAN class="ft11">Decision making: To optimize the operation of the robots, the decision needs to be made while allocating the tasks. This is done based on costs and rewards which must be designed for each task and finally for overall operation.</SPAN></P>
<P class="p6 ft7"><SPAN class="ft5">d.</SPAN><SPAN class="ft9">Task allocation and handling: This module will take care of proper allocation, operation and completion of the task allocated to the robots. In case of a fault, the module will be responsible to tackle it in an optimized and safe way.</SPAN></P>
<P class="p8 ft7"><SPAN class="ft5">e.</SPAN><SPAN class="ft9">Path planning: To optimize the fleet performance, it needs to be directed properly to right places at right times. Hence, the fleet needs an efficient routing protocol to follow. It will again save a lot of time and energy of the overall system.</SPAN></P>
<P class="p6 ft7"><SPAN class="ft2">f.</SPAN><SPAN class="ft12">Load transportation: The ground robot can operate for long periods of time, carry high payloads, perform targeted actions, such as fertilizer application or selective weed treatment, on the areas selected by the UAV.</SPAN></P>
<P class="p9 ft7"><SPAN class="ft5">B)</SPAN><SPAN class="ft6">Computer Vision in Agriculture: </SPAN>In agriculture, a robot can help to perform various tasks like planting, weeding, harvesting and plant health detection. Such robots can detect plants, weeds and fruits or vegetables with the power of analyzing the health condition and fructify level to determine the harvesting time with the reaping capability of such crops.</P>
<P class="p10 ft14"><SPAN class="ft5">a.</SPAN><SPAN class="ft13">Feature engineering: Environment like agriculture possess repetitive feature bundles that makes it hard to be tracked and detected distinctly. One needs to engineer a feature that can be calculated based on the data and can be tracked easily.</SPAN></P>
</DIV>
<DIV id="id1_2">
</DIV>
</DIV>
<DIV id="page_2">


<DIV id="id2_1">
<P class="p8 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Event detection and tracking: Events such as pest/disease detection, intruder detection needs to be tracked in real time as such events can cause plenty of damage. For this a UAV needs to map, detect, track and share the information to the ground vehicle for further treatment.</SPAN></P>
<P class="p12 ft17"><SPAN class="ft10">c.</SPAN><SPAN class="ft16">Precise localization: For anomaly detection, UAV/UGVs need precise localization of interest points and themselves. Present sensors mounted on UAVs are now capable to achieving </SPAN><NOBR>sub-pixel</NOBR> accuracy but still require heavy computation. This can be dealt with light and fast algorithms.</P>
<P class="p12 ft19"><SPAN class="ft5">d.</SPAN><SPAN class="ft18">Mapping: The robots can also cooperate to generate 3D maps of the environment annotated with parameters, such as crop density and weed pressure, suitable for supporting the farmer’s decision making. The UAV can quickly provide a coarse reconstruction of a large area, that can be updated with more detailed and higher resolution map portions generated by the UGV visiting selected areas.</SPAN></P>
<P class="p8 ft7"><SPAN class="ft5">e.</SPAN><SPAN class="ft9">Surveillance: To deal with events such as pest infestation, heavy rainfall, intruder admission, etc., the farm needs to be constantly surveyed (every 24 hours or with some defined periodicity). In these cases, a static camera or CCTV may not be able to provide adequate coverage whereas a UAV can hover and provide almost complete coverage of the field.</SPAN></P>
<P class="p8 ft7"><SPAN class="ft2">f.</SPAN><SPAN class="ft12">Visual servoing: Vision based robot control is required for precise manipulation (for instance, while carrying out spraying operation) and movement of robots in the field. This can help reduce the wastage and improve accuracy.</SPAN></P>
<P class="p13 ft7"><SPAN class="ft20">C)</SPAN><SPAN class="ft21">Machine/Deep Learning for Agriculture: </SPAN>Conventional (linear, statistical) methods fail to meet real time performance and accuracy when there are is a large multiplicity of identifiers (classes/species/diseases, etc.) to handle. These cases can be dealt with using learning techniques as they offer massive parallelization. These techniques are generally used as classifiers and predictors. Advantages of these methods (over conventional methods) can be listed as follows:</P>
<P class="p8 ft7"><SPAN class="ft5">a.</SPAN><SPAN class="ft9">Feature extraction: Extracting feature/pattern from large datasets (e.g., Hyperspectral) can be cumbersome manually and sometimes not even possible. This can be dealt with using enough feature extraction layer (e.g., CNN) and let the model learn from the data itself.</SPAN></P>
<P class="p6 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Multiple/overlapping classes: Conventional methods fails to detect patterns among highly overlapping classes or multiple classes at a time. This can be dealt with by using a probabilistic method such as used in any generic classifier.</SPAN></P>
<P class="p10 ft2"><SPAN class="ft10">c.</SPAN><SPAN class="ft11">Flexible and adaptable: Same model can be applied to different types of crops and diseases. It needs to be trained once again on new datasets. For conventional methods, new features need to be engineered to detect different diseases/crops.</SPAN></P>
<P class="p6 ft7"><SPAN class="ft5">d.</SPAN><NOBR><SPAN class="ft9">Real-time</SPAN></NOBR> and accurate: Training takes considerable time and dataset, but testing/detection is faster when compared to conventional methods which employ heavy mathematical operations (for example, PCA, SVD, etc.) and data <NOBR>pre-processing</NOBR> steps.</P>
<P class="p14 ft2"><SPAN class="ft5">e.</SPAN><SPAN class="ft22">Challenges addressed: Occlusion, depth variation, illumination, scale, etc.</SPAN></P>
<P class="p15 ft2"><SPAN class="ft2">f.</SPAN><SPAN class="ft23">Yield prediction: Yield mapping and estimation can be made to meet real time supply and demands.</SPAN></P>
<P class="p16 ft3">Work Done so Far</P>
<P class="p17 ft24">Below are the relevant projects that I have done so far, some as a part of my coursework and some for my research. The modules developed for them are listed along with the generic application. Most of the basic and specific modules developed can be employed for agricultural scenarios with little or no modifications.</P>
<P class="p18 ft7"><SPAN class="ft10">A.</SPAN><SPAN class="ft21">Indoor Localization and Path Planning: </SPAN>The project dealt with the basics of mapping, <NOBR>vision-based</NOBR> localization and path planning for a GPS denied environment.</P>
<P class="p14 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Module developed/tested: Mapping, </SPAN><NOBR>vision-based</NOBR> localization, path planning</P>
<P class="p6 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Agriculture applications: The module developed can serve as a basic module for localizing a robot and planning its path for several </SPAN><NOBR>location-based</NOBR> tasks. For instance, it can be used for navigating a UGV in a field and detecting a weed patch.</P>
</DIV>
<DIV id="id2_2">
</DIV>
</DIV>
<DIV id="page_3">


<DIV id="id3_1">
<P class="p19 ft7"><SPAN class="ft10">B.</SPAN><SPAN class="ft25">Transient dynamics analysis of foldable drone: Done under robotics course, this project aimed to analyze the dynamics of a foldable drone in transition from one state to other.</SPAN></P>
<P class="p14 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Modules developed/tested: UAV dynamics for multiple configurations</SPAN></P>
<P class="p6 ft14"><SPAN class="ft5">b.</SPAN><SPAN class="ft13">Agriculture applications: The analysis in this project can be used to analyze different possible configuration for a </SPAN><NOBR>task-oriented</NOBR> setup. For example, it can be used for analyzing a UAV with robotic arm in motion.</P>
<P class="p20 ft17"><SPAN class="ft10">C.</SPAN><SPAN class="ft26">Vehicle detection and classification based on aerial imagery: Given aerial images, the task was to detect and classify the vehicles on highway based on their size, number of wheels etc. using YOLOv3.</SPAN></P>
<P class="p14 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Modules developed/tested: YOLOv3 network architecture, training and testing setup</SPAN></P>
<P class="p21 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Agriculture applications: The network can be deployed to classify objects in the aerial images given the annotated dataset. This can be used to classify weed/diseased plants from the healthy ones (event detection.</SPAN></P>
<P class="p22 ft19"><SPAN class="ft5">D.</SPAN><SPAN class="ft27">Real time stereovision aided inertial navigation for fast autonomous flight: This work was reviewed to get accustomed to computationally efficient and real time algorithms designed to achieve fast paced flight.</SPAN></P>
<P class="p14 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Module developed/tested: Light and fast algorithm for localization and path planning</SPAN></P>
<P class="p6 ft14"><SPAN class="ft5">b.</SPAN><SPAN class="ft13">Agriculture applications: This algorithm can help UAV to travel at faster speeds simultaneously keeping track of its pose and orientation. It can help a UAV to cover larger farms with faster speed hence can provide better coverage of the same.</SPAN></P>
<P class="p23 ft7"><SPAN class="ft10">E.</SPAN><NOBR><SPAN class="ft9">Multi-agent</SPAN></NOBR> collaborative building construction: This framework was developed for MBZIRC challenge to construct a wall using 3 UAVs and 1 UGV by picking brick of desired dimension and placing it on top of another in the minimum time possible.</P>
<P class="p7 ft7"><SPAN class="ft5">a.</SPAN><SPAN class="ft9">Module developed/tested: </SPAN><NOBR>Multi-agent</NOBR> simulation setup, mission planner, task handler, collision avoidance, arm manipulation</P>
<P class="p6 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Agriculture applications: The setup can be deployed to serve the </SPAN><NOBR>task-oriented</NOBR> problems with multiple robots which collaborate with each other for data/manipulation. For instance, UAVs can detect an event/spot from above and share the location information with the UGV to operate to. Picking up agricultural tools and spray cans for delivery to another spot is another application. The mission planner would be very useful to coordinate operations among multiple vehicles.</P>
<P class="p8 ft7"><SPAN class="ft10">c.</SPAN><SPAN class="ft28">The following publication is being further developed for agricultural applications: K. Ankit, L.A. Tony, S. Jana, D. Ghose, </SPAN><NOBR>"Multi-Agent</NOBR> Collaboration for Building Construction", MBZIRC Symposium, ADNEC, Abu Dhabi, Feb 2020. <A href="https://arxiv.org/abs/2009.03584"><SPAN class="ft29">https://arxiv.org/abs/2009.03584</SPAN></A></P>
<P class="p24 ft7"><SPAN class="ft10">F.</SPAN><NOBR><SPAN class="ft9">Multi-agent</SPAN></NOBR> collaborative 3D mapping: This project aimed to fuse the map generated by a UAV and UGV separately and obtain a more precise and detailed 3D map of the environment.</P>
<P class="p14 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Modules developed/tested: </SPAN><NOBR>UAV-UGV</NOBR> collaborative mapping pipeline</P>
<P class="p21 ft7"><SPAN class="ft5">b.</SPAN><SPAN class="ft9">Agriculture applications: This pipeline can help to generate a </SPAN><NOBR>geo-referenced</NOBR> detailed 3D map along with aerial view for better analysis and operation. In an agricultural setup, a detailed map can be generated by fusing the aerial canopy view with the detailed closeup view of the rows by UGV. This can help to better detect the disease, weed, etc.</P>
<P class="p25 ft7"><SPAN class="ft10">G.</SPAN><SPAN class="ft30">Deep RL for high precision tasks: In this project, a high precision benchmark problem “Peg in hole” was solved using a deep RL architecture.</SPAN></P>
<P class="p7 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Modules developed/tested: LSTMs for precise manipulation of 6DOF arm with imprecise encoders for </SPAN><NOBR>“Peg-in-hole”</NOBR> problem</P>
<P class="p26 ft14"><SPAN class="ft5">b.</SPAN><SPAN class="ft13">Agriculture applications: This network can be employed to precisely manipulate robotic arm for tasks such as precision fertigation and irrigation using manipulator arms on the UGV.</SPAN></P>
</DIV>
<DIV id="id3_2">
</DIV>
</DIV>
<DIV id="page_4">
<DIV id="id4_1">
<P class="p27 ft7"><SPAN class="ft31">H.</SPAN><NOBR><SPAN class="ft30">DMD-MPC:</SPAN></NOBR> An online learning approach to MPC: In this work, an online learning perspective to model predictive control (MPC) has been applied and tested on <NOBR>non-linear</NOBR> control problems such as inverted pendulum and <NOBR>half-cheetah.</NOBR></P>
<P class="p14 ft2"><SPAN class="ft5">a.</SPAN><SPAN class="ft22">Modules developed/tested: Model predictive controller, </SPAN><NOBR>DMD-MPC</NOBR> algorithm</P>
<P class="p6 ft14"><SPAN class="ft5">b.</SPAN><SPAN class="ft13">Applications: This optimal controller can be deployed to predict and control the actions of a robot in a disturbance/error prone environment. An UAV can use this controller in windy conditions to properly navigate and traverse across the farm.</SPAN></P>
<P class="p28 ft7">Major requirements for automated agriculture and the modules developed can be linked and visualized using the schematic below. Black bold lined boxes are basic modules for primary tasks such as path planning, collision avoidance etc. and colored are the specific modules depending upon the specific task like collaborative 3D mapping.</P>
</DIV>
<DIV id="id4_2">
</DIV>
</DIV>
</BODY>
</HTML>
